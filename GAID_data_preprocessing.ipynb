{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Read the Excel file\n",
    "# TODO: Edit file path\n",
    "file_path = \"./data/GAIDs_merged_2025-03-24.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "total_respondents = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5196917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Country distribution (unchanged)\n",
    "country_counts = df['country'].value_counts().reset_index()\n",
    "country_counts.columns = ['country', 'count']\n",
    "print(\"Country distribution:\")\n",
    "print(country_counts)\n",
    "print(f\"Total assigned to country buckets: {country_counts['count'].sum()} of {total_respondents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Age distribution with custom buckets\n",
    "# First, determine which age group each respondent belongs to\n",
    "df['age_category'] = 'Prefer not to say'  # Default value\n",
    "\n",
    "# Check if there's a single demo_age column with the age category\n",
    "if 'demo_age' in df.columns and df['demo_age'].notna().any():\n",
    "    # Assuming demo_age has values 1-8 corresponding to age categories\n",
    "    df.loc[df['demo_age'].isin([1, 2]), 'age_category'] = '<25'  # <18 and 18-24\n",
    "    df.loc[df['demo_age'] == 3, 'age_category'] = '25-34'\n",
    "    df.loc[df['demo_age'] == 4, 'age_category'] = '35-44'\n",
    "    df.loc[df['demo_age'] == 5, 'age_category'] = '45-54'\n",
    "    df.loc[df['demo_age'].isin([6, 7, 8]), 'age_category'] = '>55'  # 55-64, 65-74, and >75\n",
    "else:\n",
    "    # Handle the case where age is coded in separate binary columns\n",
    "    for i in range(1, 3):  # <18 and 18-24 -> <25\n",
    "        col_name = f'demo_age{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'age_category'] = '<25'\n",
    "    \n",
    "    if 'demo_age3' in df.columns:  # 25-34\n",
    "        df.loc[df['demo_age3'] == 1, 'age_category'] = '25-34'\n",
    "    \n",
    "    if 'demo_age4' in df.columns:  # 35-44\n",
    "        df.loc[df['demo_age4'] == 1, 'age_category'] = '35-44'\n",
    "    \n",
    "    if 'demo_age5' in df.columns:  # 45-54\n",
    "        df.loc[df['demo_age5'] == 1, 'age_category'] = '45-54'\n",
    "    \n",
    "    for i in range(6, 9):  # 55-64, 65-74, and >75 -> >55\n",
    "        col_name = f'demo_age{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'age_category'] = '>55'\n",
    "    \n",
    "    # Mark explicit 'prefer not to say' responses\n",
    "    if 'demo_age_prefernottosay' in df.columns:\n",
    "        df.loc[df['demo_age_prefernottosay'] == 1, 'age_category'] = 'Prefer not to say'\n",
    "\n",
    "age_counts = df['age_category'].value_counts().reset_index()\n",
    "age_counts.columns = ['age_range', 'count']\n",
    "age_counts = age_counts.sort_values(by='age_range', key=lambda x: pd.Categorical(\n",
    "    x, categories=['<25', '25-34', '35-44', '45-54', '>55', 'Prefer not to say'], ordered=True\n",
    "))\n",
    "print(\"\\nAge distribution:\")\n",
    "print(age_counts)\n",
    "print(f\"Total assigned to age buckets: {age_counts['count'].sum()} of {total_respondents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb330ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gender distribution\n",
    "df['gender'] = 'Prefer not to say'  # Default for NA and explicit prefer not to say\n",
    "\n",
    "# Check if there are separate binary columns for each gender\n",
    "if 'demo_gender_female' in df.columns:\n",
    "    df.loc[df['demo_gender_female'] == 1, 'gender'] = 'Female'\n",
    "if 'demo_gender_male' in df.columns:\n",
    "    df.loc[df['demo_gender_male'] == 1, 'gender'] = 'Male'\n",
    "if 'demo_gender_diverse' in df.columns:\n",
    "    df.loc[df['demo_gender_diverse'] == 1, 'gender'] = 'Diverse'\n",
    "if 'demo_gender_prefernottosay' in df.columns:\n",
    "    df.loc[df['demo_gender_prefernottosay'] == 1, 'gender'] = 'Prefer not to say'\n",
    "# Check if there's a single demo_gender column with coded values\n",
    "elif 'demo_gender' in df.columns and df['demo_gender'].notna().any():\n",
    "    gender_mapping = {1: 'Female', 2: 'Male', 3: 'Diverse', 4: 'Prefer not to say'}\n",
    "    df['gender'] = df['demo_gender'].map(gender_mapping).fillna('Prefer not to say')\n",
    "\n",
    "gender_counts = df['gender'].value_counts().reset_index()\n",
    "gender_counts.columns = ['gender', 'count']\n",
    "gender_counts = gender_counts.sort_values(by='gender', key=lambda x: pd.Categorical(\n",
    "    x, categories=['Female', 'Male', 'Diverse', 'Prefer not to say'], ordered=True\n",
    "))\n",
    "print(\"\\nGender distribution:\")\n",
    "print(gender_counts)\n",
    "print(f\"Total assigned to gender buckets: {gender_counts['count'].sum()} of {total_respondents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64461853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Education level distribution with custom buckets\n",
    "df['education_level'] = 'Prefer not to say'  # Default for NA and explicit prefer not to say\n",
    "\n",
    "# Check if there's a demo_bachelor column that can be used directly\n",
    "if 'demo_bachelor' in df.columns and df['demo_bachelor'].notna().any():\n",
    "    # Map Yes/No values to education levels\n",
    "    education_mapping = {'yes': 'Bachelor and above', 'no': 'No university degree'}\n",
    "    df['education_level'] = df['demo_bachelor'].map(education_mapping).fillna('Prefer not to say')\n",
    "    \n",
    "    # Further differentiate between Bachelor and Master+\n",
    "    for i in range(9, 11):  # Bachelor level (9-10)\n",
    "        col_name = f'demo_education{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'education_level'] = 'Bachelor'\n",
    "    \n",
    "    for i in range(11, 14):  # Master and above (11-13)\n",
    "        col_name = f'demo_education{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'education_level'] = 'Master and above'\n",
    "else:\n",
    "    # Handle the case where education is coded in separate binary columns\n",
    "    for i in range(1, 9):  # Below Bachelor\n",
    "        col_name = f'demo_education{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'education_level'] = 'No university degree'\n",
    "    \n",
    "    for i in range(9, 11):  # Bachelor level (9-10)\n",
    "        col_name = f'demo_education{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'education_level'] = 'Bachelor'\n",
    "    \n",
    "    for i in range(11, 14):  # Master and above (11-13)\n",
    "        col_name = f'demo_education{i}'\n",
    "        if col_name in df.columns:\n",
    "            df.loc[df[col_name] == 1, 'education_level'] = 'Master and above'\n",
    "    \n",
    "    # Mark explicit 'prefer not to say' or 'other' responses\n",
    "    if 'demo_education_other' in df.columns:\n",
    "        # Keep as 'Prefer not to say' for simplicity, or create a separate category if needed\n",
    "        pass\n",
    "\n",
    "education_counts = df['education_level'].value_counts().reset_index()\n",
    "education_counts.columns = ['education_level', 'count']\n",
    "education_counts = education_counts.sort_values(by='education_level', key=lambda x: pd.Categorical(\n",
    "    x, categories=['No university degree', 'Bachelor', 'Master and above', 'Prefer not to say'], ordered=True\n",
    "))\n",
    "print(\"\\nEducation level distribution:\")\n",
    "print(education_counts)\n",
    "print(f\"Total assigned to education buckets: {education_counts['count'].sum()} of {total_respondents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completeness check for each category\n",
    "print(\"\\nCompleteness Check:\")\n",
    "print(f\"Country: {country_counts['count'].sum()/total_respondents:.2%}\")\n",
    "print(f\"Age: {age_counts['count'].sum()/total_respondents:.2%}\")\n",
    "print(f\"Gender: {gender_counts['count'].sum()/total_respondents:.2%}\")\n",
    "print(f\"Education: {education_counts['count'].sum()/total_respondents:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db735335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with just the 6 specified columns\n",
    "df_simplified = pd.DataFrame({\n",
    "    'questionnaire_id': df['questionnaire_id'],\n",
    "    'AI_tech': df['AI_tech'],\n",
    "    'country': df['country'],\n",
    "    'age': df['age_category'],\n",
    "    'gender': df['gender'],\n",
    "    'education': df['education_level']\n",
    "})\n",
    "\n",
    "# Display the first few rows to check\n",
    "print(f\"New dataframe shape: {df_simplified.shape}\")\n",
    "print(df_simplified.head())\n",
    "\n",
    "# Check for any missing values in the new dataframe\n",
    "print(\"\\nMissing values count in the simplified dataframe:\")\n",
    "print(df_simplified.isnull().sum())\n",
    "\n",
    "# Make sure all records from the original dataframe are preserved\n",
    "print(f\"\\nOriginal dataframe record count: {len(df)}\")\n",
    "print(f\"New dataframe record count: {len(df_simplified)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, add the binary feeling variables (0 or 1 coded)\n",
    "feeling_cols = [\n",
    "    'feeling_pre_hopeful', 'feeling_pre_confident', 'feeling_pre_excited', \n",
    "    'feeling_pre_relaxed', 'feeling_pre_afraid', 'feeling_pre_angry', \n",
    "    'feeling_pre_nervous', 'feeling_pre_frustrated', \n",
    "    'feeling_pre_none of the above', 'feeling_pre_Idontknow'\n",
    "]\n",
    "\n",
    "for col in feeling_cols:\n",
    "    if col in df.columns:\n",
    "        # Fill NA with 0 (assuming NA means the feeling wasn't selected)\n",
    "        df_simplified[col] = df[col].fillna(np.nan)\n",
    "    else:\n",
    "        # If the column doesn't exist, create it with all 0s\n",
    "        df_simplified[col] = np.nan\n",
    "\n",
    "# 2. Add the 1-5 scale variables with NA coded as 6 (\"I don't know\")\n",
    "scale_cols = [\n",
    "    'AIAS_life_pre', 'AIAS_work_pre', 'AIAS_futureuse_pre', 'AIAS_positive_pre',\n",
    "    'AI_interest_curiosity', 'AI_interest_general', 'AI_interest_read', 'AI_interest_watchlisten'\n",
    "]\n",
    "\n",
    "for col in scale_cols:\n",
    "    if col in df.columns:\n",
    "        # Copy the data and ensure only values 1-5 are kept\n",
    "        df_simplified[col] = df[col].copy()\n",
    "        df_simplified[col] = df_simplified[col].apply(lambda x: x if x in [1, 2, 3, 4, 5] else np.nan)\n",
    "    else:\n",
    "        # Check if there are individual columns for each scale value (e.g., AIAS_life_pre1, AIAS_life_pre2, etc.)\n",
    "        has_individual_cols = any(f\"{col}{i}\" in df.columns for i in range(1, 6))\n",
    "        \n",
    "        if has_individual_cols:\n",
    "            df_simplified[col] = np.nan  # Default to NaN\n",
    "            \n",
    "            # Assign values based on the individual columns\n",
    "            for i in range(1, 6):\n",
    "                col_name = f\"{col}{i}\"\n",
    "                if col_name in df.columns:\n",
    "                    df_simplified.loc[df[col_name] == 1, col] = i\n",
    "        else:\n",
    "            # If neither the main column nor individual columns exist, set all to NaN\n",
    "            df_simplified[col] = np.nan\n",
    "\n",
    "# Check the first few rows to see if the new columns were added correctly\n",
    "print(f\"Updated dataframe shape: {df_simplified.shape}\")\n",
    "print(df_simplified.head())\n",
    "\n",
    "# Check for any values outside the expected range for each type of column\n",
    "print(\"\\nChecking value ranges:\")\n",
    "# Binary columns should only have 0 or 1\n",
    "for col in feeling_cols:\n",
    "    unique_vals = sorted(df_simplified[col].unique())\n",
    "    print(f\"{col}: {unique_vals}\")\n",
    "\n",
    "# Scale columns should have values 1-5 (1-5 for responses, null for NA/\"I don't know\")\n",
    "for col in scale_cols:\n",
    "    unique_vals = sorted(df_simplified[col].dropna().unique(), key=lambda x: float(x) if isinstance(x, str) else x)\n",
    "    print(f\"{col}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c0537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the groups of columns for which we'll calculate means\n",
    "attitude_cols = ['AIAS_life_pre', 'AIAS_work_pre', 'AIAS_futureuse_pre', 'AIAS_positive_pre']\n",
    "interest_cols = ['AI_interest_curiosity', 'AI_interest_general', 'AI_interest_read', 'AI_interest_watchlisten']\n",
    "\n",
    "# Function to calculate mean, treating 0 (\"I don't know\") as a missing value\n",
    "def mean_excluding_dontknow(row, columns):\n",
    "    # Extract values from specified columns in this row\n",
    "    values = [row[col] for col in columns]\n",
    "    # Filter out NaN and \"don't know\" values (0)\n",
    "    valid_values = [v for v in values if pd.notna(v) and v != 0]\n",
    "    # Return mean if there are valid values, otherwise return NaN\n",
    "    return np.nanmean(valid_values) if valid_values else np.nan\n",
    "\n",
    "# Calculate means for each group\n",
    "df_simplified['AIAS_mean_pre'] = df_simplified.apply(\n",
    "    lambda row: mean_excluding_dontknow(row, attitude_cols), axis=1\n",
    ")\n",
    "\n",
    "df_simplified['AI_interest_mean'] = df_simplified.apply(\n",
    "    lambda row: mean_excluding_dontknow(row, interest_cols), axis=1\n",
    ")\n",
    "\n",
    "# Round to 2 decimal places for readability\n",
    "df_simplified['AIAS_mean_pre'] = df_simplified['AIAS_mean_pre'].round(2)\n",
    "df_simplified['AI_interest_mean'] = df_simplified['AI_interest_mean'].round(2)\n",
    "\n",
    "# Check the results\n",
    "print(\"\\nSummary of mean columns:\")\n",
    "print(f\"AIAS_mean_pre: {df_simplified['AIAS_mean_pre'].describe()}\")\n",
    "print(f\"AI_interest_mean: {df_simplified['AI_interest_mean'].describe()}\")\n",
    "\n",
    "# Check for missing values in the mean columns\n",
    "print(\"\\nMissing values in mean columns:\")\n",
    "print(f\"AIAS_mean_pre: {df_simplified['AIAS_mean_pre'].isna().sum()}\")\n",
    "print(f\"AI_interest_mean: {df_simplified['AI_interest_mean'].isna().sum()}\")\n",
    "\n",
    "# Sample a few rows to verify the calculation\n",
    "print(\"\\nSample verification:\")\n",
    "sample_indices = df_simplified.sample(3).index\n",
    "for idx in sample_indices:\n",
    "    row = df_simplified.loc[idx]\n",
    "    print(f\"\\nRow {idx}:\")\n",
    "    \n",
    "    # Print AIAS values and mean\n",
    "    print(\"AIAS values:\", end=\" \")\n",
    "    for col in attitude_cols:\n",
    "        print(f\"{col}={row[col]}\", end=\" \")\n",
    "    print(f\"→ Mean: {row['AIAS_mean_pre']}\")\n",
    "    \n",
    "    # Print interest values and mean\n",
    "    print(\"Interest values:\", end=\" \")\n",
    "    for col in interest_cols:\n",
    "        print(f\"{col}={row[col]}\", end=\" \")\n",
    "    print(f\"→ Mean: {row['AI_interest_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the simplified dataframe to JSON format\n",
    "# Using 'records' orientation to create an array of objects (one per row)\n",
    "df_simplified.to_json('gaid_data.json', orient='records')\n",
    "\n",
    "# For better human readability (with indentation), you can use:\n",
    "import json\n",
    "with open('gaid_data_readable.json', 'w', encoding='utf-8') as f:\n",
    "    json_str = df_simplified.to_json(orient='records')\n",
    "    json_data = json.loads(json_str)\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Data exported to JSON successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59452c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
